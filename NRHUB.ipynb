{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import keras\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers \n",
    "import numpy as np\n",
    "from numpy.linalg import cholesky\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.optimizers import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "with open('DocMeta2.tsv') as f:\n",
    "    newsdata=f.readlines()\n",
    "\n",
    "\n",
    "news={}\n",
    "category={}\n",
    "subcategory={}\n",
    "for i in newsdata:\n",
    "    line=i.strip().split('\\t')\n",
    "    if len(line)<5:\n",
    "        continue\n",
    "    news[line[1]]=[line[2],line[3],word_tokenize(line[6].lower())]\n",
    "    if line[2] not in category:\n",
    "        category[line[2]]=len(category)\n",
    "    if line[3] not in subcategory:\n",
    "        subcategory[line[3]]=len(subcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "with open('UserSQUET.tsv')as f:\n",
    "    dataq=f.readlines()\n",
    "\n",
    "with open('ClickData2.tsv')as f:\n",
    "    user=f.readlines()\n",
    "userid_dict={}\n",
    "for i in user:\n",
    "    line=i.strip().split('\\t')\n",
    "    userid=line[0]\n",
    "    if userid not in userid_dict:\n",
    "        userid_dict[userid]=len(userid_dict)\n",
    "\n",
    "userdata={}\n",
    "for i in dataq:\n",
    "    line=i.replace('\\n','').split('\\t')\n",
    "    query=[word_tokenize(x.split('#TAB#')[0].lower()) for x in line[2].split('#N#')]\n",
    "    web=[word_tokenize(x.split('#TAB#')[0].lower()) for x in line[3].split('#N#')]\n",
    "    userdata[line[0]]=[query,web]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "raw_word_dict={'PADDING':[0,999999]}\n",
    "\n",
    "for i in news:\n",
    "    for word in news[i][2]:\n",
    "        if word in raw_word_dict:\n",
    "            raw_word_dict[word][1]+=1\n",
    "        else:\n",
    "            raw_word_dict[word]=[len(raw_word_dict),1]\n",
    "for i in userdata:\n",
    "    for query in userdata[i][0]:\n",
    "        for word in query:\n",
    "            if word in raw_word_dict:\n",
    "                raw_word_dict[word][1]+=1\n",
    "            else:\n",
    "                raw_word_dict[word]=[len(raw_word_dict),1]\n",
    "for i in userdata:\n",
    "    for title in userdata[i][1]:\n",
    "        for word in title:\n",
    "            if word in raw_word_dict:\n",
    "                raw_word_dict[word][1]+=1\n",
    "            else:\n",
    "                raw_word_dict[word]=[len(raw_word_dict),1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "for i in raw_word_dict:\n",
    "    if raw_word_dict[i][1]>=3:\n",
    "        word_dict[i]=[len(word_dict),raw_word_dict[i][1]]\n",
    "print(len(word_dict),len(raw_word_dict))\n",
    "\n",
    "\n",
    "\n",
    "embdict={}\n",
    "plo=0\n",
    "import pickle\n",
    "with open('glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        j=f.readline()\n",
    "        if len(j)==0:\n",
    "            break\n",
    "        k = j.split()\n",
    "        word=k[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            line=[float(x) for x in k[1:]]\n",
    "            if word in word_dict:\n",
    "                embdict[word]=line\n",
    "                if plo%100==0:\n",
    "                    print(plo,linenb,word)\n",
    "                plo+=1\n",
    "\n",
    "\n",
    "\n",
    "word_dict1=word_dict\n",
    "print(len(embdict),len(word_dict1))\n",
    "print(len(word_dict1))\n",
    "embmat=[0]*len(word_dict1)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict.keys():\n",
    "    embmat[word_dict1[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(embmat[word_dict1[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(embmat)):\n",
    "    if type(embmat[i])==int:\n",
    "        embmat[i]=np.reshape(norm, 300)\n",
    "embmat[0]=np.zeros(300,dtype='float32')\n",
    "embmat=np.array(embmat,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "news_words = [[0]*30]\n",
    "news_index = {'0':0}\n",
    "for i in news:\n",
    "    line = []\n",
    "    news_index[i] = len(news_index)\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict:\n",
    "            line.append(word_dict[j][0])\n",
    "    line = line[:30]\n",
    "    news_words.append(line+[0]*(30-len(line)))\n",
    "\n",
    "news_words=np.array(news_words,dtype='int32') \n",
    "\n",
    "\n",
    "\n",
    "user_query = []\n",
    "user_title = []\n",
    "MAXQ=300\n",
    "MAXT=300\n",
    "MAXQL=10\n",
    "MAXTL=25\n",
    "\n",
    "for i in userid_dict:\n",
    "    \n",
    "    if i in userdata:\n",
    "        tq=[]\n",
    "        tt=[]\n",
    "        for j in userdata[i][0]:\n",
    "            \n",
    "            tqq=[word_dict[m][0] for m in j if m in word_dict][:MAXQL]\n",
    "            tq.append(tqq+[0]*(MAXQL-len(tqq)))\n",
    "        tq=tq[:MAXQ]\n",
    "        user_query.append(tq+[[0]*MAXQL]*(MAXQ-len(tq)))\n",
    "        for j in userdata[i][1]:\n",
    "            \n",
    "            ttt=[word_dict[m][0] for m in j if m in word_dict][:MAXTL]\n",
    "            tt.append(ttt+[0]*(MAXTL-len(ttt)))\n",
    "        tt=tt[:MAXT]\n",
    "        user_title.append(tt+[[0]*MAXTL]*(MAXT-len(tt)))\n",
    "    else:\n",
    "        user_query.append([[0]*MAXQL]*MAXQ)\n",
    "        user_title.append([[0]*MAXTL]*MAXT)\n",
    "\n",
    "user_query=np.array(user_query,dtype='int32') \n",
    "user_title=np.array(user_title,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "def newsample(neglist,ratio):\n",
    "    if ratio >len(neglist):\n",
    "        return random.sample(neglist*(ratio//len(neglist)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(neglist,ratio)\n",
    "\n",
    "npratio=4\n",
    "train_userid=[]\n",
    "train_candidate=[]    \n",
    "train_label=[]\n",
    "\n",
    "test_userid=[]\n",
    "test_candidate=[]    \n",
    "test_label=[]\n",
    "all_test_index=[]\n",
    "\n",
    "train_history=[]\n",
    "test_history=[]\n",
    "\n",
    "for raw in user:\n",
    "    line=raw.strip().split('\\t')\n",
    "    userid=line[0]\n",
    "    if len(line)==4:\n",
    "        \n",
    "        impr=[x.split('#TAB#') for x in line[2].split('#N#')]\n",
    "    if len(line)==3:\n",
    "        impr=[x.split('#TAB#') for x in line[2].split('#N#')]\n",
    "        \n",
    "\n",
    "    \n",
    "    trainpos=[x[0].split() for x in impr]\n",
    "    trainneg=[x[1].split() for x in impr]\n",
    "    \n",
    " \n",
    "    poslist=list(itertools.chain(*(trainpos)))\n",
    "    neglist=list(itertools.chain(*(trainneg)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(line)==4:\n",
    "        impr=[x.split('#TAB#') for x in line[3].split('#N#')]\n",
    "        rawtestpos=[x[0].split() for x in impr] \n",
    "        rawtestneg=[x[1].split() for x in impr] \n",
    "        \n",
    "        testpos=[]\n",
    "        testneg=[]\n",
    "        for k in range(len(rawtestpos)):\n",
    " \n",
    "            testpos.append(rawtestpos[k])\n",
    "            testneg.append(rawtestneg[k])\n",
    "        \n",
    "        for i in range(len(testpos)):\n",
    "            currentindex=[]\n",
    "            currentindex.append(len(test_candidate))\n",
    "            history=random.sample(poslist,min(50,len(poslist)))[:50]\n",
    "            history+=[0]*(50-len(history))\n",
    "    \n",
    "            for j in testpos[i]:\n",
    "                test_candidate.append(news_index[j])\n",
    "                test_label.append(1)\n",
    "                test_userid.append(userid_dict[userid])\n",
    "                test_history.append(history)\n",
    "                \n",
    "            for j in testneg[i]:\n",
    "                test_candidate.append(news_index[j])\n",
    "                test_label.append(0)\n",
    "                test_userid.append(userid_dict[userid])\n",
    "                test_history.append(history)\n",
    "            currentindex.append(len(test_candidate))\n",
    "            all_test_index.append(currentindex)\n",
    "            \n",
    "\n",
    " \n",
    "    for posindex in range(len(trainpos)):\n",
    "        for posdoc in trainpos[posindex]:\n",
    "            \n",
    "            candidate_list=newsample(trainneg[posindex],npratio)\n",
    "            candidate_list.append(posdoc)\n",
    "            tplb=[0]*npratio+[1]\n",
    "            tid=list(range(npratio+1))\n",
    "            random.shuffle(tid)\n",
    "            yp=[]\n",
    "            yplb=[]\n",
    "            for j in tid:\n",
    "                yp.append(news_index[candidate_list[j]])\n",
    "                yplb.append(tplb[j])\n",
    "            \n",
    "            poslist_reduced=list(set(poslist)-set([posdoc]))\n",
    "            history=random.sample(poslist_reduced,min(50,len(poslist_reduced)))[:50]\n",
    "            history+=[0]*(50-len(history))\n",
    "            train_candidate.append(yp)\n",
    "            train_label.append(yplb)\n",
    "            train_userid.append(userid_dict[userid])\n",
    "            train_history.append(history)\n",
    "\n",
    "\n",
    "\n",
    "train_candidate=np.array(train_candidate,dtype='int32')\n",
    "train_label=np.array(train_label,dtype='int32')\n",
    "train_userid=np.array(train_userid,dtype='int32')\n",
    "test_candidate=np.array(test_candidate,dtype='int32')\n",
    "test_label=np.array(test_label,dtype='int32')\n",
    "test_userid=np.array(test_userid,dtype='int32')\n",
    "train_history=np.array(train_history,dtype='int32')\n",
    "test_history=np.array(test_history,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "def generate_batch_data_random(train_candidate,train_label,train_userid,batch_size):\n",
    "    idx = np.arange(len(train_label))\n",
    "    np.random.shuffle(idx)\n",
    "    y=train_label\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = news_words[train_candidate[i]]\n",
    "            user=news_words[train_history[i]]\n",
    "            label=train_label[i]\n",
    "            \n",
    "            yield ([item,user,user_query[train_userid[i]],user_title[train_userid[i]]], [label])\n",
    "\n",
    "\n",
    "def generate_batch_data(train_candidate,train_label,train_userid,batch_size):\n",
    "    idx = np.arange(len(train_label))\n",
    "\n",
    "    y=train_label\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = news_words[train_candidate[i]]\n",
    "            user=news_words[test_history[i]]\n",
    "            label=train_label[i]\n",
    "            \n",
    "            yield ([item,user,user_query[train_userid[i]],user_title[train_userid[i]]], [label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], 1)))\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        eij = K.dot(x, self.W)\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai / K.expand_dims(K.sum(ai, axis=1), 1)\n",
    "        print('weights', weights.shape)\n",
    "        weighted_input = x * weights\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50\n",
    "MAXQ=300\n",
    "MAXT=300\n",
    "MAXQL=10\n",
    "MAXTL=25\n",
    "results=[]\n",
    "for npratio in range(4,5):\n",
    "    keras.backend.clear_session()\n",
    "    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    embedding_layer = Embedding(len(word_dict), 300, weights=[embmat],trainable=True) \n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    d_emb=Dropout(0.2)(embedded_sequences)\n",
    "    \n",
    "    news_cnn = Convolution1D(nb_filter=400, filter_length=3,  padding='same', activation='relu', strides=1)(d_emb)\n",
    "    d_news_cnn=Dropout(0.2)(news_cnn)\n",
    "    newsrep=AttLayer()(Dense(200,activation='tanh')(d_news_cnn))\n",
    "    news_encoder = Model([sentence_input], newsrep)\n",
    "    news_input = keras.Input((MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32')\n",
    "    newsreps = TimeDistributed(news_encoder)(news_input)\n",
    "    userrep_news=AttLayer()(Dense(200,activation='tanh')(newsreps))\n",
    "    \n",
    "    query_input = Input(shape=(MAXQL,), dtype='int32')\n",
    "    query_embedded_sequences = embedding_layer(query_input)\n",
    "    d_query_embedded_sequences=Dropout(0.2)(query_embedded_sequences)\n",
    "    query_cnn = Convolution1D(nb_filter=400, filter_length=3,  padding='same', activation='relu', strides=1)(d_query_embedded_sequences)\n",
    "    d_query_cnn=Dropout(0.2)(query_cnn)\n",
    "    queryrep=AttLayer()(Dense(200,activation='tanh')(d_query_cnn))\n",
    "    query_encoder = Model([query_input], queryrep)\n",
    "    queries_input = Input(shape=(MAXQ,MAXQL,), dtype='int32') \n",
    "    queryreps = TimeDistributed(query_encoder)(queries_input)\n",
    "    userrep_query=AttLayer()(Dense(200,activation='tanh')(queryreps))\n",
    "    \n",
    "    title_input = Input(shape=(MAXTL,), dtype='int32')\n",
    "    title_embedded_sequences = embedding_layer(title_input)\n",
    "    d_title_embedded_sequences=Dropout(0.2)(title_embedded_sequences)\n",
    "    title_cnn = Convolution1D(nb_filter=400, filter_length=3,  padding='same', activation='relu', strides=1)(d_title_embedded_sequences)\n",
    "    d_title_cnn=Dropout(0.2)(title_cnn)\n",
    "    titlerep=AttLayer()(Dense(200,activation='tanh')(d_title_cnn))\n",
    "    title_encoder = Model([title_input], titlerep)\n",
    "    titles_input = Input(shape=(MAXT,MAXTL,), dtype='int32') \n",
    "    titlereps = TimeDistributed(title_encoder)(titles_input)\n",
    "    userrep_title=AttLayer()(Dense(200,activation='tanh')(titlereps))\n",
    "\n",
    "    uservecs =concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(vec) for vec in [userrep_news,userrep_title,userrep_query]],axis=1)\n",
    "    l_atttv=AttLayer()(Dense(200,activation='tanh')(uservecs))\n",
    "    \n",
    "    candidates = keras.Input((1+npratio,MAX_SENT_LENGTH,), dtype='int32')\n",
    "    candidate_vecs = TimeDistributed(news_encoder)(candidates)\n",
    "    logits = keras.layers.dot([l_atttv, candidate_vecs], axes=-1)\n",
    "    logits = keras.layers.Activation(keras.activations.softmax)(logits)\n",
    "    model = Model([candidates,news_input,queries_input,titles_input], [logits])\n",
    "    \n",
    "    candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "    candidate_one_vec = news_encoder([candidate_one,user_id])\n",
    "    score = keras.layers.Activation(keras.activations.sigmoid)(\n",
    "        keras.layers.dot([l_atttv, candidate_one_vec], axes=-1))\n",
    "    model_test = keras.Model([candidate_one,news_input,queries_input,titles_input], score)\n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "    for epoch in range(2):\n",
    "        traingen=generate_batch_data_random(train_candidate,train_label,train_userid, 30)\n",
    "        model.fit_generator(traingen, epochs=1,steps_per_epoch=len(train_userid)//30)\n",
    "        valgen=generate_batch_data(test_candidate,test_label,test_userid, 50)\n",
    "        pred = model_test.predict_generator(valgen, steps=len(test_userid)//50,verbose=1)\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        all_auc=[]\n",
    "        all_mrr=[]\n",
    "        all_ndcg=[]\n",
    "        all_ndcg2=[]\n",
    "        for index in all_test_index:\n",
    "            if np.sum(test_label[index[0]:index[1]])!=0 and index[1]<len(pred):\n",
    "                all_auc.append(roc_auc_score(test_label[index[0]:index[1]],pred[index[0]:index[1],0]))\n",
    "                all_mrr.append(mrr_score(test_label[index[0]:index[1]],pred[index[0]:index[1],0]))\n",
    "                all_ndcg.append(ndcg_score(test_label[index[0]:index[1]],pred[index[0]:index[1],0],k=5))\n",
    "                all_ndcg2.append(ndcg_score(test_label[index[0]:index[1]],pred[index[0]:index[1],0],k=10))\n",
    "        results.append([np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)])\n",
    "        print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb（副本）（副本）",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
